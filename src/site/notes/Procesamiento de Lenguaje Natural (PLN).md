---
{"dg-publish":true,"permalink":"/procesamiento-de-lenguaje-natural-pln/","dgPassFrontmatter":true}
---

# ğŸ§  **Generalidades**

## ğŸš€ **Â¿QuÃ© es el PLN?**
El **Procesamiento de Lenguaje Natural (PLN)** (en inglÃ©s, **Natural Language Processing - NLP**) es un campo de la inteligencia artificial (IA) que permite a las mÃ¡quinas **comprender, interpretar, generar y responder** en lenguaje humano de manera natural y significativa.

El PLN combina conceptos de:  
âœ… **LingÃ¼Ã­stica computacional** (estructura del lenguaje)  
âœ… **Machine Learning** (aprendizaje a partir de datos)  
âœ… **Deep Learning** (redes neuronales profundas)  

---

## ğŸ¯ **Objetivo del PLN**
El objetivo principal es hacer que las computadoras:  
- Comprendan el lenguaje humano (texto y voz)  
- Generen respuestas coherentes  
- InteractÃºen de manera natural con las personas  

---

## ğŸ“Œ **Componentes Clave del PLN**
### 1. **TokenizaciÃ³n**
Dividir el texto en unidades mÃ¡s pequeÃ±as (**tokens**).  
**Ejemplo:**  
"El gato estÃ¡ en el sofÃ¡." â†’ ["El", "gato", "estÃ¡", "en", "el", "sofÃ¡", "."]  

---

### 2. **LematizaciÃ³n y Stemming**
- **Stemming:** Reduce las palabras a su raÃ­z o forma base.  
  - "Jugando" â†’ "Jug"  
- **LematizaciÃ³n:** Reduce las palabras a su forma base teniendo en cuenta el contexto gramatical.  
  - "Jugando" â†’ "Jugar"  

---

### 3. **Etiquetado gramatical (POS Tagging)**
Identifica la categorÃ­a gramatical de cada token.  
**Ejemplo:**  
- "El" â†’ artÃ­culo  
- "gato" â†’ sustantivo  
- "estÃ¡" â†’ verbo  

---

### 4. **Reconocimiento de entidades (NER)**  
Identifica nombres propios, fechas, organizaciones, etc.  
**Ejemplo:**  
*"Apple anunciÃ³ el nuevo iPhone el 12 de septiembre."*  
- Apple â†’ OrganizaciÃ³n  
- iPhone â†’ Producto  
- 12 de septiembre â†’ Fecha  

---

### 5. **AnÃ¡lisis de dependencias (Dependency Parsing)**  
Analiza la estructura gramatical de la oraciÃ³n para determinar quÃ© palabra depende de cuÃ¡l.  

**Ejemplo:**  
*"El gato estÃ¡ en el sofÃ¡."*  
- "estÃ¡" â†’ verbo principal  
- "gato" â†’ sujeto  
- "sofÃ¡" â†’ objeto  

---

### 6. **AnÃ¡lisis de sentimiento**  
EvalÃºa el tono emocional del texto.  
- "Â¡Me encanta este producto!" â†’ Sentimiento positivo  
- "El servicio fue terrible." â†’ Sentimiento negativo  

---

### 7. **GeneraciÃ³n de texto**  
Modelos como **GPT** o **BERT** pueden generar texto coherente en respuesta a una entrada especÃ­fica.  
**Ejemplo:**  
Entrada â†’ "Â¿CuÃ¡l es la capital de Francia?"  
Salida â†’ "La capital de Francia es ParÃ­s."  

---

## ğŸ” **Embeddings**
Un **embedding** es una representaciÃ³n vectorial de una palabra o token en un espacio multidimensional.  
- Los embeddings permiten que el modelo capture relaciones semÃ¡nticas y sintÃ¡cticas entre palabras.  
- Palabras con significados similares tendrÃ¡n embeddings cercanos en el espacio vectorial.  

### âœ… **Ejemplo:**  
- "Rey" estarÃ¡ cerca de "reina" y "prÃ­ncipe" en el espacio vectorial.  
- La relaciÃ³n "rey - hombre + mujer â‰ˆ reina" funciona porque las palabras estÃ¡n organizadas por significado en el espacio vectorial.  

---

## ğŸ“Œ **Tipos de embeddings**
### âœ… **Word2Vec** (2013)  
- Usa un modelo de redes neuronales para mapear palabras en un espacio vectorial.  
- Capta relaciones semÃ¡nticas bÃ¡sicas ("rey" â†’ "reina").  

### âœ… **GloVe** (2014)  
- Usa la co-ocurrencia de palabras en un corpus grande para construir embeddings.  
- RepresentaciÃ³n global del lenguaje.  

### âœ… **FastText** (2016)  
- Similar a Word2Vec, pero aÃ±ade subpalabras (permite capturar significado de palabras desconocidas).  

### âœ… **Embeddings Contextuales (BERT, GPT)**  
- Los embeddings varÃ­an segÃºn el contexto de la frase.  
- "Banco" puede tener embeddings distintos segÃºn si se refiere a una instituciÃ³n financiera o a un objeto fÃ­sico para sentarse.  

---

## ğŸ§  **Mecanismo de AtenciÃ³n**
El mecanismo de atenciÃ³n permite que el modelo analice las relaciones entre palabras dentro de una oraciÃ³n para modificar los embeddings y reflejar el contexto.

### âœ… Â¿CÃ³mo funciona?  
1. Cada token genera tres vectores:  
   - **Q (Query):** Lo que busca el token.  
   - **K (Key):** InformaciÃ³n ofrecida por el token.  
   - **V (Value):** El valor que aporta el token al significado final.  

2. La atenciÃ³n calcula una combinaciÃ³n ponderada usando:  
$$
\text{Attention} = \text{softmax} \left( \frac{Q K^T}{\sqrt{d_k}} \right) V
$$

3. El resultado modifica los valores de las coordenadas del embedding para reflejar el contexto.

---

### ğŸ”¥ **Ejemplo de AtenciÃ³n:**
*"El gato estÃ¡ en el sofÃ¡."*  
- La atenciÃ³n permite que "estÃ¡" se asocie con "gato" y "sofÃ¡" para captar el contexto de que un gato estÃ¡ en un sofÃ¡.  
- El embedding de "estÃ¡" cambia para reflejar el sujeto ("gato") y el objeto ("sofÃ¡").  

---

## ğŸ“Œ **Magnitud vs DirecciÃ³n**
- El mecanismo de atenciÃ³n **no cambia directamente la magnitud** del vector, pero puede afectarla indirectamente.  
- El foco principal estÃ¡ en **cambiar la direcciÃ³n** (es decir, las coordenadas) del vector para reflejar el contexto.  

---

## ğŸ“Œ **Â¿Las Dimensiones del Embedding Representan Significado?**
- Las dimensiones **no tienen significados explÃ­citos** como "gÃ©nero", "animal", "acciÃ³n".  
- Sin embargo, las relaciones emergen durante el entrenamiento:  
    - "Rey - hombre + mujer â‰ˆ reina"  
- El modelo organiza automÃ¡ticamente el espacio vectorial para que estas relaciones se reflejen de manera latente.  

---

## ğŸŒ **Aplicaciones del PLN**
âœ… **TraducciÃ³n automÃ¡tica** â†’ Google Translate  
âœ… **Chatbots** â†’ ChatGPT  
âœ… **Resumen automÃ¡tico** â†’ Resumir noticias  
âœ… **AnÃ¡lisis de sentimiento** â†’ Evaluar comentarios de clientes  
âœ… **Reconocimiento de voz** â†’ Siri, Alexa  
âœ… **GeneraciÃ³n de texto** â†’ GPT, BERT  
âœ… **BÃºsqueda semÃ¡ntica** â†’ Google Search  

---

## ğŸš¨ **DesafÃ­os del PLN**
ğŸš¨ **AmbigÃ¼edad:** La misma palabra puede tener mÃºltiples significados.  
ğŸš¨ **Contexto:** Comprender el contexto completo de una conversaciÃ³n o texto largo.  
ğŸš¨ **IronÃ­a y sarcasmo:** DifÃ­ciles de captar para las mÃ¡quinas.  
ğŸš¨ **Lenguaje informal:** Slang, emojis y lenguaje coloquial.  

---

## ğŸš€ **Resumen Clave**
âœ… El texto â†’ tokens â†’ embeddings iniciales  
âœ… El mecanismo de atenciÃ³n cambia las coordenadas (no solo la magnitud)  
âœ… Los embeddings modificados reflejan el contexto semÃ¡ntico  
âœ… Las relaciones complejas emergen en el espacio vectorial sin que las dimensiones estÃ©n definidas manualmente  

---

