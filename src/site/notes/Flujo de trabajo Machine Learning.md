---
{"dg-publish":true,"permalink":"/flujo-de-trabajo-machine-learning/","dgPassFrontmatter":true}
---


Una gu√≠a pr√°ctica para preparar correctamente los datos antes de entrenar modelos de clasificaci√≥n (con especial foco en redes neuronales, SVM y modelos sensibles al escalado).

---

## üìå 1. Divisi√≥n del dataset

**Objetivo:** Separar el dataset en conjuntos que no compartan informaci√≥n.

```python
from sklearn.model_selection import train_test_split

# Divisi√≥n en entrenamiento (60%), validaci√≥n (20%) y prueba (20%)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
```

---

## üìå 2. Preprocesamiento seg√∫n tipo de variable

### üî∑ Variables continuas
- Escalado recomendado:
  - `StandardScaler` si los datos tienen distribuci√≥n normal.
  - `MinMaxScaler` si tienen un rango definido.
  - `RobustScaler` si hay valores at√≠picos.

### üî∑ Variables ordinales
- Pueden escalarse como si fueran continuas.
- Escalado sugerido: `MinMaxScaler` o `StandardScaler`.

### üî∑ Variables categ√≥ricas
- Convertir a **One-Hot Encoding** si no tienen orden l√≥gico.

```python
import pandas as pd
data = pd.get_dummies(data, columns=['MARRIAGE'], drop_first=True)
```

- Para variables binarias (ej. SEX: 1 = masculino, 2 = femenino), se puede binarizar:

```python
data['SEX'] = data['SEX'].map({1: 0, 2: 1})
```

---

## üìå 3. Escalado de datos

‚ö†Ô∏è **Solo se debe ajustar el escalador con el conjunto de entrenamiento** para evitar **fugas de datos**.

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)
```

---

## üìå 4. Opcional: Preprocesamiento con `ColumnTransformer`

Permite aplicar transformaciones espec√≠ficas por tipo de variable:

```python
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler

preprocessor = ColumnTransformer(transformers=[
    ('num', StandardScaler(), ['AGE', 'LIMIT_BAL']),
    ('cat', 'passthrough', ['SEX', 'MARRIAGE_1', 'MARRIAGE_2'])
])

X_train_scaled = preprocessor.fit_transform(X_train)
X_val_scaled = preprocessor.transform(X_val)
X_test_scaled = preprocessor.transform(X_test)
```

---

## üìå 5. Balanceo de clases (si es necesario)

Cuando las clases est√°n desbalanceadas (por ejemplo, muchos ejemplos de clase 0 y pocos de clase 1), el modelo tiende a favorecer la clase mayoritaria. Aqu√≠ tienes las t√©cnicas m√°s utilizadas para corregir esto:
### Opci√≥n 1: SMOTE (Sobremuestreo sint√©tico)

**SMOTE** (Synthetic Minority Over-sampling Technique) crea ejemplos sint√©ticos para la clase minoritaria a partir de combinaciones lineales de ejemplos reales cercanos.

```python
from imblearn.over_sampling import SMOTE
smote = SMOTE(sampling_strategy=0.8, random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
```
- ‚úÖ Aumenta la clase minoritaria sin duplicar ejemplos.
    
- ‚ö†Ô∏è Puede generar puntos irreales si la distribuci√≥n original es muy dispersa.
    
- Ideal para modelos sensibles al balance como MLP o SVM.
### Opci√≥n 2: Submuestreo aleatorio

Elimina ejemplos aleatorios de la clase mayoritaria para igualar la cantidad de ejemplos con la clase minoritaria.

```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler(random_state=42)
X_resampled, y_resampled = rus.fit_resample(X_train, y_train)
```
- ‚úÖ Simple y r√°pido.
    
- ‚ö†Ô∏è Puede eliminar informaci√≥n importante de la clase mayoritaria.
    
- √ötil si tienes muchos datos o poca memoria.

### Opci√≥n 3: SMOTE + Tomek (t√©cnica h√≠brida)

**SMOTETomek** combina SMOTE (para generar ejemplos de la clase minoritaria) y **Tomek Links** (para eliminar ejemplos ruidosos de la clase mayoritaria que est√°n demasiado cerca de los de la clase minoritaria).
 ```python
 from imblearn.combine import SMOTETomek
smote_tomek = SMOTETomek(sampling_strategy=0.8, random_state=42)
X_resampled, y_resampled = smote_tomek.fit_resample(X_train, y_train)
```
- ‚úÖ Mejora el balance y tambi√©n **limpia el l√≠mite entre clases**.
    
- ‚ö†Ô∏è Puede eliminar ejemplos valiosos si el l√≠mite entre clases es difuso.
    
- Ideal para modelos que se benefician de una buena separaci√≥n de clases (como redes neuronales o SVM).

### ü§® ¬øCu√°l elegir?

| T√©cnica       | Pros                                     | Contras                             | Ideal para...           |
| ------------- | ---------------------------------------- | ----------------------------------- | ----------------------- |
| SMOTE         | Crea ejemplos sin eliminar datos         | Puede crear ruido                   | MLP, SVM                |
| Undersampling | Simple, reduce tiempo de entrenamiento   | Pierde informaci√≥n                  | √Årboles, Random Forest  |
| SMOTE + Tomek | Balancea y limpia el l√≠mite entre clases | Puede ser agresivo eliminando datos | Modelos complejos (MLP) |

---
## ‚úÖ Buenas pr√°cticas finales

- üîí **Evita fuga de datos**: nunca uses info del test o validaci√≥n para escalar o balancear.


